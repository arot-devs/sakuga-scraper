{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic scraper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping post ID: 272528\n",
      "Successfully downloaded post 272528\n",
      "Scraping post ID: 272541\n",
      "Successfully downloaded post 272541\n",
      "Scraping post ID: 272539\n",
      "Successfully downloaded post 272539\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class SakugaScraper:\n",
    "    BASE_URL = \"https://www.sakugabooru.com/post/show/{}\"\n",
    "\n",
    "    def __init__(self, root_dir: str):\n",
    "        self.root_dir = root_dir\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "    def fetch_post(self, post_id: str):\n",
    "        url = self.BASE_URL.format(post_id)\n",
    "        response = httpx.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    def extract_metadata(self, soup, post_id: str):\n",
    "        metadata = {}\n",
    "        # Extract high-res image link\n",
    "        highres_link = soup.find(\"a\", id=\"highres\")\n",
    "        metadata[\"image_url\"] = highres_link[\"href\"] if highres_link else None\n",
    "\n",
    "        # Extract tags\n",
    "        tag_sidebar = soup.find(\"ul\", id=\"tag-sidebar\")\n",
    "        if tag_sidebar:\n",
    "            for li in tag_sidebar.find_all(\"li\"):\n",
    "                tag_type = li.get(\"class\", [None])[0]\n",
    "                # Find the second <a> tag (the one with the actual name)\n",
    "                tag_name = None\n",
    "                a_tags = li.find_all(\"a\")\n",
    "                if len(a_tags) > 1:\n",
    "                    tag_name = a_tags[1].text.strip()\n",
    "                elif a_tags:\n",
    "                    tag_name = a_tags[0].text.strip()\n",
    "                if tag_name:\n",
    "                    metadata.setdefault(tag_type, []).append(tag_name)\n",
    "        metadata[\"post_id\"] = post_id\n",
    "        return metadata\n",
    "\n",
    "\n",
    "    def download_image(self, url: str, save_path: str):\n",
    "        response = httpx.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    def scrape_post(self, post_id: str):\n",
    "        soup = self.fetch_post(post_id)\n",
    "        metadata = self.extract_metadata(soup, post_id)\n",
    "\n",
    "        # Prepare directories and file paths\n",
    "        post_dir = os.path.join(self.root_dir, f\"post_{post_id}\")\n",
    "        os.makedirs(post_dir, exist_ok=True)\n",
    "        ext = metadata[\"image_url\"].split(\".\")[-1] if metadata[\"image_url\"] else \"jpg\"\n",
    "        image_path = os.path.join(post_dir, f\"sankaku_{post_id}.{ext}\")\n",
    "        metadata_path = os.path.join(post_dir, f\"sankaku_{post_id}.json\")\n",
    "\n",
    "        # Download image and save metadata\n",
    "        if metadata[\"image_url\"]:\n",
    "            self.download_image(metadata[\"image_url\"], image_path)\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "\n",
    "    def scrape_posts(self, post_ids: list[str]):\n",
    "        for post_id in post_ids:\n",
    "            try:\n",
    "                print(f\"Scraping post ID: {post_id}\")\n",
    "                self.scrape_post(post_id)\n",
    "                print(f\"Successfully downloaded post {post_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download post {post_id}: {e}\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = SakugaScraper(root_dir=\"sakuga_downloads\")\n",
    "    scraper.scrape_posts([\"272528\", \"272541\", \"272539\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scraper with more meta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping post ID: 44843\n",
      "Successfully downloaded post 44843\n",
      "Scraping post ID: 272528\n",
      "Successfully downloaded post 272528\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class SakugaScraper:\n",
    "    BASE_URL = \"https://www.sakugabooru.com/post/show/{}\"\n",
    "\n",
    "    def __init__(self, root_dir: str):\n",
    "        self.root_dir = root_dir\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "    def fetch_post(self, post_id: str):\n",
    "        \"\"\"Fetch the HTML content of the given post ID.\"\"\"\n",
    "        url = self.BASE_URL.format(post_id)\n",
    "        response = httpx.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, \"html.parser\"), url\n",
    "\n",
    "    def extract_metadata(self, soup, post_id: str, post_url: str):\n",
    "        \"\"\"Extract metadata from the post HTML.\"\"\"\n",
    "        metadata = {\"post_id\": post_id, \"post_url\": post_url}\n",
    "\n",
    "        # Extract high-res image link\n",
    "        highres_link = soup.find(\"a\", id=\"highres\")\n",
    "        metadata[\"image_url\"] = highres_link[\"href\"] if highres_link else None\n",
    "\n",
    "        # Extract tags\n",
    "        metadata[\"tags\"] = {}\n",
    "        tag_sidebar = soup.find(\"ul\", id=\"tag-sidebar\")\n",
    "        if tag_sidebar:\n",
    "            for li in tag_sidebar.find_all(\"li\"):\n",
    "                tag_type = li.get(\"class\", [None])[0]\n",
    "                tag_name = li.find_all(\"a\")[1].text.strip() if li.find_all(\"a\") else None\n",
    "                if tag_name:\n",
    "                    metadata[\"tags\"].setdefault(tag_type, []).append(tag_name)\n",
    "\n",
    "        # Extract statistics\n",
    "        stats = soup.find(\"div\", id=\"stats\")\n",
    "        if stats:\n",
    "            for li in stats.find_all(\"li\"):\n",
    "                text = li.text.strip()\n",
    "                if \":\" in text:\n",
    "                    key, value = map(str.strip, text.split(\":\", 1))\n",
    "                    metadata[key.lower().replace(\" \", \"_\")] = value\n",
    "\n",
    "        # Extract status notices\n",
    "        metadata[\"status_notice\"] = []\n",
    "        metadata[\"status_notice_parsed\"] = {}\n",
    "        status_notices = soup.find_all(\"div\", class_=\"status-notice\")\n",
    "        for notice in status_notices:\n",
    "            notice_text = notice.text.strip()\n",
    "            metadata[\"status_notice\"].append(notice_text)\n",
    "\n",
    "            # Parse parent post\n",
    "            if \"belongs to a parent post\" in notice_text.lower():\n",
    "                parent_link = notice.find(\"a\", href=True)\n",
    "                if parent_link:\n",
    "                    metadata[\"status_notice_parsed\"][\"parent_post_id\"] = parent_link[\"href\"].split(\"/\")[-1]\n",
    "\n",
    "            # Parse deletion flag\n",
    "            if \"flagged for deletion\" in notice_text.lower():\n",
    "                flagger_info = notice_text.split(\"Reason:\")\n",
    "                if len(flagger_info) > 1:\n",
    "                    metadata[\"status_notice_parsed\"][\"deletion_reason\"] = flagger_info[1].strip()\n",
    "                flagged_by = notice_text.split(\"by \")\n",
    "                if len(flagged_by) > 1:\n",
    "                    metadata[\"status_notice_parsed\"][\"flagged_by\"] = flagged_by[1].split(\".\")[0].strip()\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def download_image(self, url: str, save_path: str):\n",
    "        \"\"\"Download an image from the given URL.\"\"\"\n",
    "        response = httpx.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    def scrape_post(self, post_id: str):\n",
    "        \"\"\"Scrape a single post by ID.\"\"\"\n",
    "        soup, post_url = self.fetch_post(post_id)\n",
    "        metadata = self.extract_metadata(soup, post_id, post_url)\n",
    "\n",
    "        # Prepare directories and file paths\n",
    "        post_dir = os.path.join(self.root_dir, f\"post_{post_id}\")\n",
    "        os.makedirs(post_dir, exist_ok=True)\n",
    "        ext = metadata[\"image_url\"].split(\".\")[-1] if metadata[\"image_url\"] else \"jpg\"\n",
    "        image_path = os.path.join(post_dir, f\"sankaku_{post_id}.{ext}\")\n",
    "        metadata_path = os.path.join(post_dir, f\"sankaku_{post_id}.json\")\n",
    "\n",
    "        # Download image and save metadata\n",
    "        if metadata[\"image_url\"]:\n",
    "            self.download_image(metadata[\"image_url\"], image_path)\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "\n",
    "    def scrape_posts(self, post_ids: list[str]):\n",
    "        \"\"\"Scrape multiple posts.\"\"\"\n",
    "        for post_id in post_ids:\n",
    "            try:\n",
    "                print(f\"Scraping post ID: {post_id}\")\n",
    "                self.scrape_post(post_id)\n",
    "                print(f\"Successfully downloaded post {post_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download post {post_id}: {e}\")\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = SakugaScraper(root_dir=\"sakuga_downloads\")\n",
    "    scraper.scrape_posts([\"44843\", \"272528\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
