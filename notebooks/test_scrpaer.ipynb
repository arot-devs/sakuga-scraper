{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class SakugaScraper:\n",
    "    BASE_URL = \"https://www.sakugabooru.com/post/show/{}\"\n",
    "\n",
    "    def __init__(self, root_dir: str):\n",
    "        self.root_dir = root_dir\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "    def fetch_post(self, post_id: str):\n",
    "        url = self.BASE_URL.format(post_id)\n",
    "        response = httpx.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    def extract_metadata(self, soup, post_id: str):\n",
    "        metadata = {}\n",
    "        # Extract high-res image link\n",
    "        highres_link = soup.find(\"a\", id=\"highres\")\n",
    "        metadata[\"image_url\"] = highres_link[\"href\"] if highres_link else None\n",
    "\n",
    "        # Extract tags\n",
    "        tag_sidebar = soup.find(\"ul\", id=\"tag-sidebar\")\n",
    "        if tag_sidebar:\n",
    "            for li in tag_sidebar.find_all(\"li\"):\n",
    "                tag_type = li.get(\"class\", [None])[0]\n",
    "                tag_name = li.find(\"a\").text.strip() if li.find(\"a\") else \"?\"\n",
    "                metadata.setdefault(tag_type, []).append(tag_name)\n",
    "        metadata[\"post_id\"] = post_id\n",
    "        return metadata\n",
    "\n",
    "    def download_image(self, url: str, save_path: str):\n",
    "        response = httpx.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    def scrape_post(self, post_id: str):\n",
    "        soup = self.fetch_post(post_id)\n",
    "        metadata = self.extract_metadata(soup, post_id)\n",
    "\n",
    "        # Prepare directories and file paths\n",
    "        post_dir = os.path.join(self.root_dir, f\"post_{post_id}\")\n",
    "        os.makedirs(post_dir, exist_ok=True)\n",
    "        ext = metadata[\"image_url\"].split(\".\")[-1] if metadata[\"image_url\"] else \"jpg\"\n",
    "        image_path = os.path.join(post_dir, f\"sankaku_{post_id}.{ext}\")\n",
    "        metadata_path = os.path.join(post_dir, f\"sankaku_{post_id}.json\")\n",
    "\n",
    "        # Download image and save metadata\n",
    "        if metadata[\"image_url\"]:\n",
    "            self.download_image(metadata[\"image_url\"], image_path)\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "\n",
    "    def scrape_posts(self, post_ids: list[str]):\n",
    "        for post_id in post_ids:\n",
    "            try:\n",
    "                print(f\"Scraping post ID: {post_id}\")\n",
    "                self.scrape_post(post_id)\n",
    "                print(f\"Successfully downloaded post {post_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download post {post_id}: {e}\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = SakugaScraper(root_dir=\"sakuga_downloads\")\n",
    "    scraper.scrape_posts([\"272528\", \"272541\", \"272539\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
